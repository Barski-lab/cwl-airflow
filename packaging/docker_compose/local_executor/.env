# We always read configuration from AIRFLOW_HOME/airflow.cfg
# We expect that your AIRFLOW_HOME folder has the following
# structure. If not, it will be created this way. Optionally,
# CWL_TMP_FOLDER, CWL_INPUTS_FOLDER, CWL_OUTPUTS_FOLDER and
# CWL_PICKLE_FOLDER folders can be also placed there if it
# was such configured.
#
#  ├── dags
#  ├── logs
#  │   ├── dag_processor_manager
#  │   │   └── dag_processor_manager.log
#  │   └── scheduler
#  └── plugins
#
# regardless of what is set in airflow.cfg for
#
#  [core]
#    dags_folder
#    base_log_folder
#    dag_processor_manager_log_location
#    plugins_folder
#  [scheduler]
#    scheduler_child_process_log_directory
# 
# as these parameters will be overwritten through the environment
# variables with their defaults for Airflow values. We can't read
# them from airflow.cfg because, for example, dags_folder should
# be mounted to the container before it is started. All log related
# parameters also should be redefined to point to locations within
# docker container, because in the airflow.cfg they might have
# locations from the host system.


# Perhaps, with docker.sock we need to optionally pass DOCKER_API_VERSION
# somehow, because docker-cli inside container can use a different API
# version from what the docker daemon on the host machine uses.

# All volumes required by CWL-Airflow will be mounted with exactly the same
# locations inside docker containers as they are on the host machine.
# CWL_TMP_FOLDER and CWL_INPUTS_FOLDER should be accessible from the inner
# container when cwltool executes workflow step. The mount path should
# correspond to the path on the host machine as we are using docker.sock
# from the host machine. For CWL_OUTPUTS_FOLDER the path inside containers
# should also be identical to the one on the host machine as the reported
# results in the JSON format better to have proper locations. AIRFLOW_HOME
# and CWL_PICKLE_FOLDER folders are mounted with the same principle only for
# consistency reason. If PROCESS_REPORT_URL is not "" the correspondent
# connection will be created and CWL-Airflow will try to report workflow
# execution progress, statuses and reports.
# You can use AIRFLOW_ENV_FILE parameter to set path to the environment file
# that will include parameters used by Airflow. If no additional parameters
# needed provide path to the empty file. Docker-compose will fail to start
# if file doesn't exist. This file may look like as follows:
# AIRFLOW__CORE__PARALLELISM=1
# AIRFLOW__CORE__DAG_CONCURRENCY=1

# CWL-Airflow
AIRFLOW_HOME=/Users/tester/compose_airflow
CWL_TMP_FOLDER=/Users/tester/compose_airflow/cwl_tmp_folder
CWL_INPUTS_FOLDER=/Users/tester/compose_airflow/cwl_inputs_folder
CWL_OUTPUTS_FOLDER=/Users/tester/compose_airflow/cwl_outputs_folder
CWL_PICKLE_FOLDER=/Users/tester/compose_airflow/cwl_pickle_folder
AIRFLOW_WEBSERVER_PORT=8080
CWL_AIRFLOW_API_PORT=8081
PROCESS_REPORT_URL=""
AIRFLOW_ENV_FILE=airflow_settings.env


# Database
MYSQL_ROOT_PASSWORD=admin
MYSQL_DATABASE=airflow
MYSQL_USER=airflow
MYSQL_PASSWORD=airflow
MYSQL_PORT=6603
MYSQL_DATA=/Users/tester/compose_airflow/mysql_data