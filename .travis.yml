sudo: false
language: python

dist: bionic
os:
- linux

services:
  - docker

python:
- 3.7

env:
- NTEST=1
- NTEST=2
- NTEST=3
- NTEST=4
- NTEST=5
- NTEST=6
- NTEST=7
- NTEST=8
- NTEST=9
- NTEST=10
- NTEST=11
- NTEST=12
- NTEST=13
- NTEST=14
- NTEST=15
- NTEST=16
- NTEST=17
- NTEST=18
- NTEST=19
- NTEST=20
- NTEST=21
- NTEST=22
- NTEST=23
- NTEST=24
- NTEST=25
- NTEST=26
- NTEST=27
- NTEST=28
- NTEST=29
- NTEST=30
- NTEST=31
- NTEST=32

jobs:
  include:
  - name: Unit tests
    before_install:   # to overwrite from the main config
    - echo "Skip"
    install:
    - pip install . --constraint ./packaging/constraints/constraints-$TRAVIS_PYTHON_VERSION.txt
    before_script:    # to overwrite from the main config
    - echo "Skip"
    script: ./tests/run_tests.sh
    after_success:
    - coveralls
    deploy:
      provider: pypi
      user: michael_kotliar
      password:
        secure: Mji1koR4nyt/KgoycpuvgIp9toFVNYaSxUmNY6EVt0pmIpRb/GKbw6TdyfAdtnSAwH3BcSUC/R1hCwyaXfv1GDPFYqv9Yg1MaNHR1clvo8E8KIIPt1JDqPM47lgPQQFFbwB+Cc6uSV0Nn9oDBkhWEPQqV3kI/GJkSUzSs/yjZqR4C+aZxsJzE+VX2ZzeGCD3x4mzhAAWan4MLrdgANaXQVTHhyHIhTp3l109FblYimMvx8HqKotMiM+32mVFxgwf/pMw/N8gDOFXd4VrtlaOqqHpn4VJko+jSNYuAdKn62N2KFKqExyU39ycvU9ngYaU38nmCjJdibRgNyxfdH6LfndS9xzu3KPY64ACLG1i8Ym+57Q7wSJZAb2WF/b8av1RnkKMUGHHYXBzVIGk7Abvuhde0DsV0lr9XsapQn7XySmhdBWYazZTr+AtgIdsx7AmHV1ug6nPp3tIQzW1+YAOf295Puwqbrn+SF3jYw6167jAl5M1a81kxqli1UTsLgpcaTbTD1ofwLn4gP3VuU1f4fKGzhrxl6ybHW+LpO/wkcN2wJDdBbqz5OQIYfshMQEooIODOw1OonmwbY3vcMATuvi7Hz3mIElqpu3TVxH9aoBzcvL1148wPhZF8u87T8nDgsHeUT66I56ILGcZszASolt2Cb6oPZmxg2jgajTREwk=
      on:
        tags: true
  - name: DAG with embedded workflow (just one test)
    script: cwl-airflow test --suite workflows/tests/conformance_tests.yaml --spin --range 1 --embed
  - name: DAG with attached workflow using combined API call (just one test)
    script: cwl-airflow test --suite workflows/tests/conformance_tests.yaml --spin --range 1 --combine
  - name: DAG with embedded workflow using combined API call (just one test)
    script: cwl-airflow test --suite workflows/tests/conformance_tests.yaml --spin --range 1 --embed --combine
  - name: Test of `init --upgrade`
    before_install:
    - mkdir -p ~/airflow/dags
    - cp ./tests/data/dags/bam_bedgraph_bigwig_single_old_format.py ~/airflow/dags
    - cp ./tests/data/workflows/bam-bedgraph-bigwig-single.cwl ~/airflow/dags
    install:
    - pip install . --constraint ./packaging/constraints/constraints-$TRAVIS_PYTHON_VERSION.txt
    before_script:
    - cwl-airflow init --upgrade
    - rm -f ~/airflow/dags/bam-bedgraph-bigwig-single.cwl
    script: airflow dags list  # to check if all DAGs are correct
  - name: Test packaging for Ubuntu 18.04, Python 3.6
    install:
    - ./packaging/portable/ubuntu/pack.sh 18.04 3.6 $TRAVIS_BRANCH
    - ls ./packaging/portable/ubuntu/build/
    - tar xzf "./packaging/portable/ubuntu/build/python_3.6_with_cwl_airflow_${TRAVIS_BRANCH}_ubuntu_18.04.tar.gz"
    before_script:
    - ./python3/bin_portable/airflow --help             # to generate airflow.cfg
    - sed -i'.backup' -e 's/^executor.*/executor = LocalExecutor/g' ~/airflow/airflow.cfg
    - sed -i'.backup' -e 's/^dag_dir_list_interval =.*/dag_dir_list_interval = 60/g' ~/airflow/airflow.cfg
    - sed -i'.backup' -e 's/^parallelism =.*/parallelism = 1/g' ~/airflow/airflow.cfg
    - sed -i'.backup' -e 's/^sql_alchemy_conn.*/sql_alchemy_conn = mysql:\/\/airflow:airflow@127.0.0.1:6603\/airflow/g' ~/airflow/airflow.cfg
    - ./python3/bin_portable/cwl-airflow init           # to init database
    - ./python3/bin_portable/airflow connections add process_report --conn-type http --conn-host localhost --conn-port 3070 --conn-extra "{\"endpoint\":\"/airflow/\"}"    # to add process_report connection
    - ./python3/bin_portable/airflow scheduler > /dev/null 2>&1 &
    - ./python3/bin_portable/cwl-airflow api > /dev/null 2>&1 &
    script: ./python3/bin_portable/cwl-airflow test --suite workflows/tests/conformance_tests.yaml --spin --range 1

before_install:
- git clone https://github.com/datirium/workflows.git --recursive
- docker pull mysql/mysql-server:5.7
- docker run -v ~/database:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=airflow -e MYSQL_DATABASE=airflow -e MYSQL_USER=airflow -e MYSQL_PASSWORD=airflow -p 6603:3306 -d mysql/mysql-server:5.7 --explicit-defaults-for-timestamp=1
install:
- pip install ".[mysql,crypto]" --constraint ./packaging/constraints/constraints-$TRAVIS_PYTHON_VERSION.txt
before_script:
- airflow --help             # to generate airflow.cfg
- sed -i'.backup' -e 's/^executor.*/executor = LocalExecutor/g' ~/airflow/airflow.cfg
- sed -i'.backup' -e 's/^dag_dir_list_interval =.*/dag_dir_list_interval = 60/g' ~/airflow/airflow.cfg
- sed -i'.backup' -e 's/^parallelism =.*/parallelism = 1/g' ~/airflow/airflow.cfg
- sed -i'.backup' -e 's/^sql_alchemy_conn.*/sql_alchemy_conn = mysql:\/\/airflow:airflow@127.0.0.1:6603\/airflow/g' ~/airflow/airflow.cfg
- cwl-airflow init           # to init database
- airflow connections add process_report --conn-type http --conn-host localhost --conn-port 3070 --conn-extra "{\"endpoint\":\"/airflow/\"}"    # to add process_report connection
- airflow scheduler > /dev/null 2>&1 &
- cwl-airflow api > /dev/null 2>&1 &
script: cwl-airflow test --suite workflows/tests/conformance_tests.yaml --spin --range $NTEST

branches:
  only:
  - master
  - /^*_devel$/
  - /^([1-9]\d*!)?(0|[1-9]\d*)(\.(0|[1-9]\d*))*((a|b|rc)(0|[1-9]\d*))?(\.post(0|[1-9]\d*))?(\.dev(0|[1-9]\d*))?$/

notifications:
  email: false
